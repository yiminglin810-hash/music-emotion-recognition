{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MuSe Dataset: Feature Extraction\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Task Overview\n",
    "\n",
    "This notebook extracts audio features and prepares baseline models for the MuSe dataset.\n",
    "\n",
    "### ðŸŽ¯ Objectives\n",
    "1. Match audio files with V-A-D annotations\n",
    "2. Extract traditional audio features (MFCC, Chroma, Spectral, Rhythm)\n",
    "3. Train baseline regression models\n",
    "4. Compare with DEAM dataset results\n",
    "\n",
    "### ðŸ“Š Data\n",
    "- **Audio Count**: 953 songs (cleaned)\n",
    "- **Audio Duration**: 30 seconds\n",
    "- **Annotation Dimensions**: Valence, Arousal, Dominance (V-A-D)\n",
    "- **Data Source**: `audio_clips_1000/` + `muse_sampled_1000.csv`\n",
    "\n",
    "### ðŸ¤– Models\n",
    "- Linear Regression\n",
    "- Ridge Regression\n",
    "- Random Forest\n",
    "\n",
    "### ðŸ“ˆ Evaluation Metrics\n",
    "- **RÂ²** (Coefficient of Determination)\n",
    "- **MSE** (Mean Squared Error)\n",
    "- **MAE** (Mean Absolute Error)\n",
    "- **RMSE** (Root Mean Squared Error)\n",
    "- **CCC** (Concordance Correlation Coefficient)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "import librosa.display\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "# Add project path to import custom modules\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import custom feature extraction module (using same 133-dim features as DEAM)\n",
    "from src.features.traditional import extract_all_features, get_feature_names\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Random seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Visualization settings\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Librosa version:\", librosa.__version__)\n",
    "print(\"Random seed:\", RANDOM_STATE)\n",
    "print(\"\\nðŸŽ¯ Feature extraction: Using 133-dim features (same as DEAM)\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Match Data\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 1. Set paths\n",
    "AUDIO_DIR = Path('../data/MuSe/audio_clips_1000')\n",
    "ANNOTATION_PATH = Path('../data/MuSe/processed/muse_sampled_1000.csv')\n",
    "OUTPUT_DIR = Path('../data/MuSe/processed')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Load annotation data\n",
    "df_annotations = pd.read_csv(ANNOTATION_PATH)\n",
    "print(f\"Loaded annotations: {len(df_annotations)} records\")\n",
    "\n",
    "# 3. Get all audio files\n",
    "audio_files = sorted(AUDIO_DIR.glob('*.mp3'))\n",
    "print(f\"Found audio files: {len(audio_files)}\")\n",
    "\n",
    "# 4. Match audio and annotations\n",
    "matched_data = []\n",
    "for audio_file in audio_files:\n",
    "    # Extract index (0001 -> 1)\n",
    "    file_idx = int(audio_file.stem.split('-')[0])\n",
    "    \n",
    "    # Get corresponding annotation\n",
    "    if file_idx <= len(df_annotations):\n",
    "        row = df_annotations.iloc[file_idx - 1]\n",
    "        \n",
    "        matched_data.append({\n",
    "            'file_path': str(audio_file),\n",
    "            'file_name': audio_file.name,\n",
    "            'song_id': file_idx,\n",
    "            'track': row['track'],\n",
    "            'artist': row['artist'],\n",
    "            'valence': row['valence_tags'],\n",
    "            'arousal': row['arousal_tags'],\n",
    "            'dominance': row['dominance_tags'],\n",
    "            'genre': row['genre'],\n",
    "            'spotify_id': row['spotify_id']\n",
    "        })\n",
    "\n",
    "# Create matched DataFrame\n",
    "df_matched = pd.DataFrame(matched_data)\n",
    "print(f\"Successfully matched: {len(df_matched)} records\")\n",
    "\n",
    "# 5. Save matched results\n",
    "matched_output_path = OUTPUT_DIR / 'muse_audio_matched.csv'\n",
    "df_matched.to_csv(matched_output_path, index=False)\n",
    "\n",
    "# 6. Display statistics\n",
    "print(f\"\\nðŸ“Š V-A-D Statistics:\")\n",
    "print(f\"   Valence   - Mean: {df_matched['valence'].mean():.3f}, Std: {df_matched['valence'].std():.3f}, Range: [{df_matched['valence'].min():.2f}, {df_matched['valence'].max():.2f}]\")\n",
    "print(f\"   Arousal   - Mean: {df_matched['arousal'].mean():.3f}, Std: {df_matched['arousal'].std():.3f}, Range: [{df_matched['arousal'].min():.2f}, {df_matched['arousal'].max():.2f}]\")\n",
    "print(f\"   Dominance - Mean: {df_matched['dominance'].mean():.3f}, Std: {df_matched['dominance'].std():.3f}, Range: [{df_matched['dominance'].min():.2f}, {df_matched['dominance'].max():.2f}]\")\n",
    "\n",
    "# 7. Display first 5 records\n",
    "print(f\"\\nFirst 5 matched records:\")\n",
    "df_matched[['file_name', 'artist', 'track', 'valence', 'arousal', 'dominance']].head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Using same 133-dim feature extraction function as DEAM\n",
    "# This function is defined in src/features/traditional.py\n",
    "\n",
    "print(\"\\nðŸŽ¯ Using 133-dim feature extraction (same as DEAM):\")\n",
    "print(\"\\nðŸ“Š Feature Type Distribution:\")\n",
    "print(\"   â€¢ MFCC:\")\n",
    "print(\"     - mfcc_mean: 20 dims\")\n",
    "print(\"     - mfcc_std: 20 dims\")\n",
    "print(\"     - mfcc_delta_mean: 20 dims (first-order delta)\")\n",
    "print(\"     Subtotal: 60 dims\")\n",
    "print(\"\\n   â€¢ Chroma:\")\n",
    "print(\"     - chroma_stft_mean: 12 dims\")\n",
    "print(\"     - chroma_stft_std: 12 dims\")\n",
    "print(\"     - chroma_cqt_mean: 12 dims (Constant-Q Transform)\")\n",
    "print(\"     - chroma_cqt_std: 12 dims\")\n",
    "print(\"     Subtotal: 48 dims\")\n",
    "print(\"\\n   â€¢ Spectral:\")\n",
    "print(\"     - spectral_centroid: mean + std = 2 dims\")\n",
    "print(\"     - spectral_rolloff: mean + std = 2 dims\")\n",
    "print(\"     - spectral_bandwidth: mean + std = 2 dims\")\n",
    "print(\"     - zero_crossing_rate: mean + std = 2 dims\")\n",
    "print(\"     - spectral_contrast: 7 bands Ã— 2 = 14 dims\")\n",
    "print(\"     Subtotal: 22 dims\")\n",
    "print(\"\\n   â€¢ Rhythm:\")\n",
    "print(\"     - tempo: 1 dim\")\n",
    "print(\"     - beat_count: 1 dim\")\n",
    "print(\"     - beat_strength: 1 dim\")\n",
    "print(\"     Subtotal: 3 dims\")\n",
    "print(\"\\nâœ… Total: 60 + 48 + 22 + 3 = 133 dims\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Feature Extraction\n",
    "\n",
    "**âš ï¸ Note**: This step takes approximately **45-60 minutes** (953 songs Ã— 3s/song)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f\"Total songs: {len(df_matched)}\")\n",
    "print(f\"Estimated time: ~{len(df_matched) * 4 / 60:.0f} minutes (about 4 seconds per song for 133-dim features)\")\n",
    "\n",
    "# Store features\n",
    "all_features = []\n",
    "failed_count = 0\n",
    "\n",
    "# Batch extraction\n",
    "for idx, row in tqdm(df_matched.iterrows(), total=len(df_matched), desc=\"Extracting features\"):\n",
    "    try:\n",
    "        # Use extract_all_features to extract 133-dim features\n",
    "        features_dict = extract_all_features(\n",
    "            audio_path=row['file_path'],\n",
    "            sr=22050,\n",
    "            hop_length=512,\n",
    "            n_mfcc=20  # DEAM uses 20 MFCC coefficients\n",
    "        )\n",
    "        \n",
    "        if features_dict is not None and len(features_dict) > 0:\n",
    "            # Create record\n",
    "            record = {\n",
    "                'song_id': row['song_id'],\n",
    "                'file_name': row['file_name'],\n",
    "                'valence': row['valence'],\n",
    "                'arousal': row['arousal'],\n",
    "                'dominance': row['dominance']\n",
    "            }\n",
    "            \n",
    "            # Add all features\n",
    "            record.update(features_dict)\n",
    "            \n",
    "            all_features.append(record)\n",
    "        else:\n",
    "            failed_count += 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        failed_count += 1\n",
    "\n",
    "# Create feature DataFrame\n",
    "df_features = pd.DataFrame(all_features)\n",
    "\n",
    "# Calculate feature count\n",
    "feature_cols = [c for c in df_features.columns if c not in ['song_id', 'file_name', 'valence', 'arousal', 'dominance']]\n",
    "n_features = len(feature_cols)\n",
    "\n",
    "print(f\"\\nâœ… Feature extraction complete\")\n",
    "print(f\"   Success: {len(df_features)} songs\")\n",
    "print(f\"   Failed: {failed_count} songs\")\n",
    "print(f\"   Feature dimensions: {n_features} dims\")\n",
    "\n",
    "# Verify feature count\n",
    "if n_features == 133:\n",
    "    print(f\"\\nâœ… Feature count verified: {n_features} dims = 133 dims (same as DEAM)\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Feature count: {n_features} dims (expected 133 dims)\")\n",
    "\n",
    "# Save features\n",
    "features_output_path = OUTPUT_DIR / 'muse_features_all_133dim.csv'\n",
    "df_features.to_csv(features_output_path, index=False)\n",
    "print(f\"\\nðŸ’¾ Saved features to: {features_output_path}\")\n",
    "\n",
    "# Display preview\n",
    "print(f\"\\nFeature data preview:\")\n",
    "print(df_features[['song_id', 'file_name', 'valence', 'arousal', 'dominance'] + feature_cols[:5]].head())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Important Update: 133-Dimensional Features\n",
    "\n",
    "**Upgrade Notice:**\n",
    "\n",
    "This notebook has been upgraded to extract **133-dimensional features** (same as DEAM), replacing the previous 75-dimensional features.\n",
    "\n",
    "### ðŸ“Š Comparison\n",
    "\n",
    "| Version | Feature Count | Filename | Notes |\n",
    "|---------|--------------|----------|-------|\n",
    "| **Old** | 75 dims | `muse_features_all.csv` | Simplified features |\n",
    "| **New** | 133 dims | `muse_features_all_133dim.csv` | âœ… Current, same as DEAM |\n",
    "\n",
    "### âœ… Advantages\n",
    "\n",
    "1. **Fair comparison**: Same feature extraction method as DEAM\n",
    "2. **Richer information**:\n",
    "   - MFCC Delta (captures dynamic changes)\n",
    "   - Chroma CQT (more accurate pitch features)\n",
    "   - Beat Strength (rhythm intensity)\n",
    "3. **Stronger representation**: 133 dims > 75 dims\n",
    "\n",
    "### ðŸ“ Expected Results\n",
    "\n",
    "Even with 133 dims, MuSe's RÂ² is expected to remain far lower than DEAM because:\n",
    "- **Label quality** is the main bottleneck (social tags vs expert annotations)\n",
    "- Feature count increase may bring **minor improvement** (RÂ² = 0.02 â†’ 0.03-0.04)\n",
    "- But still cannot reach DEAM's level (RÂ² = 0.51)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Prepare features and labels\n",
    "non_feature_cols = ['song_id', 'file_name', 'valence', 'arousal', 'dominance']\n",
    "feature_cols = [c for c in df_features.columns if c not in non_feature_cols]\n",
    "\n",
    "print(f\"Total columns: {len(df_features.columns)}\")\n",
    "print(f\"Feature columns: {len(feature_cols)}\")\n",
    "\n",
    "X = df_features[feature_cols].values\n",
    "y_valence = df_features['valence'].values\n",
    "y_arousal = df_features['arousal'].values\n",
    "y_dominance = df_features['dominance'].values\n",
    "\n",
    "print(f\"\\nðŸ“Š Data dimensions:\")\n",
    "print(f\"   Feature matrix X: {X.shape}\")\n",
    "print(f\"   Target Valence: {y_valence.shape}\")\n",
    "print(f\"   Target Arousal: {y_arousal.shape}\")\n",
    "print(f\"   Target Dominance: {y_dominance.shape}\")\n",
    "\n",
    "# Split data: 70% train, 15% validation, 15% test\n",
    "X_temp, X_test, y_v_temp, y_v_test, y_a_temp, y_a_test, y_d_temp, y_d_test = train_test_split(\n",
    "    X, y_valence, y_arousal, y_dominance, \n",
    "    test_size=0.15, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "X_train, X_val, y_v_train, y_v_val, y_a_train, y_a_val, y_d_train, y_d_val = train_test_split(\n",
    "    X_temp, y_v_temp, y_a_temp, y_d_temp,\n",
    "    test_size=0.176,  # 0.176 * 0.85 â‰ˆ 0.15\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset sizes:\")\n",
    "print(f\"   Train:      {X_train.shape[0]} songs ({X_train.shape[0]/len(df_features)*100:.1f}%)\")\n",
    "print(f\"   Validation: {X_val.shape[0]} songs ({X_val.shape[0]/len(df_features)*100:.1f}%)\")\n",
    "print(f\"   Test:       {X_test.shape[0]} songs ({X_test.shape[0]/len(df_features)*100:.1f}%)\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Standardization\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Fit scaler on training set\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Standardization complete\")\n",
    "print(f\"\\nScaled feature statistics (training set):\")\n",
    "print(f\"   Mean: {X_train_scaled.mean():.6f}\")\n",
    "print(f\"   Std: {X_train_scaled.std():.6f}\")\n",
    "print(f\"   Min: {X_train_scaled.min():.3f}\")\n",
    "print(f\"   Max: {X_train_scaled.max():.3f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Quality Diagnostics\n",
    "\n",
    "Check feature and label quality before training models to ensure no anomalies.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Feature statistics\n",
    "print(\"ðŸ“Š Feature Statistics:\")\n",
    "print(f\"   Feature count: {X_train_scaled.shape[1]}\")\n",
    "print(f\"   Sample count: {X_train_scaled.shape[0]}\")\n",
    "print(f\"   Feature range: min={X_train_scaled.min():.4f}, max={X_train_scaled.max():.4f}\")\n",
    "print(f\"   Feature mean: {X_train_scaled.mean():.4f}\")\n",
    "print(f\"   Feature std: {X_train_scaled.std():.4f}\")\n",
    "print(f\"   Contains NaN: {np.isnan(X_train_scaled).any()}\")\n",
    "print(f\"   Contains Inf: {np.isinf(X_train_scaled).any()}\")\n",
    "\n",
    "# Label statistics\n",
    "print(\"\\nðŸ“Š Label Statistics:\")\n",
    "for target_name, target_data in [\n",
    "    ('Valence', y_v_train), \n",
    "    ('Arousal', y_a_train), \n",
    "    ('Dominance', y_d_train)\n",
    "]:\n",
    "    print(f\"\\n   {target_name}:\")\n",
    "    print(f\"      Mean: {target_data.mean():.3f}\")\n",
    "    print(f\"      Std: {target_data.std():.3f}\")\n",
    "    print(f\"      Range: [{target_data.min():.2f}, {target_data.max():.2f}]\")\n",
    "    print(f\"      Variance: {target_data.var():.3f}\")\n",
    "    print(f\"      Median: {np.median(target_data):.3f}\")\n",
    "\n",
    "# Train/test distribution comparison\n",
    "print(\"\\nðŸ“Š Train/Test Distribution Comparison:\")\n",
    "print(f\"\\n   Valence:\")\n",
    "print(f\"      Train mean: {y_v_train.mean():.3f} | Test mean: {y_v_test.mean():.3f}\")\n",
    "print(f\"      Train std: {y_v_train.std():.3f} | Test std: {y_v_test.std():.3f}\")\n",
    "\n",
    "print(f\"\\n   Arousal:\")\n",
    "print(f\"      Train mean: {y_a_train.mean():.3f} | Test mean: {y_a_test.mean():.3f}\")\n",
    "print(f\"      Train std: {y_a_train.std():.3f} | Test std: {y_a_test.std():.3f}\")\n",
    "\n",
    "print(f\"\\n   Dominance:\")\n",
    "print(f\"      Train mean: {y_d_train.mean():.3f} | Test mean: {y_d_test.mean():.3f}\")\n",
    "print(f\"      Train std: {y_d_train.std():.3f} | Test std: {y_d_test.std():.3f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Baseline Test: Mean Predictor\n",
    "\n",
    "Use the simplest predictor (directly predict mean) as baseline to see if our models are better.\n",
    "\n",
    "**Theoretically:**\n",
    "- Mean predictor's RÂ² = 0\n",
    "- If model RÂ² > 0, the model is effective\n",
    "- If model RÂ² < 0, the model is worse than mean prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "baseline_results = []\n",
    "\n",
    "for target_name, y_train, y_test in [\n",
    "    ('Valence', y_v_train, y_v_test),\n",
    "    ('Arousal', y_a_train, y_a_test),\n",
    "    ('Dominance', y_d_train, y_d_test)\n",
    "]:\n",
    "    # Mean predictor\n",
    "    dummy = DummyRegressor(strategy='mean')\n",
    "    dummy.fit(X_train_scaled, y_train)\n",
    "    y_pred = dummy.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š {target_name} (Mean Predictor):\")\n",
    "    print(f\"   RÂ²: {r2:.4f}\")\n",
    "    print(f\"   RMSE: {rmse:.4f}\")\n",
    "    print(f\"   Prediction: all {y_pred[0]:.3f} (training mean)\")\n",
    "    print(f\"   True range: [{y_test.min():.2f}, {y_test.max():.2f}]\")\n",
    "    \n",
    "    baseline_results.append({\n",
    "        'Target': target_name,\n",
    "        'RÂ²': r2,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae\n",
    "    })\n",
    "\n",
    "df_baseline = pd.DataFrame(baseline_results)\n",
    "display(df_baseline)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Train Baseline Models\n",
    "\n",
    "Now train actual machine learning models and compare with the mean predictor above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def concordance_correlation_coefficient(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Lin's Concordance Correlation Coefficient (CCC)\n",
    "    \n",
    "    CCC is a standard evaluation metric in emotion recognition tasks,\n",
    "    used to measure agreement between predicted and true values.\n",
    "    \n",
    "    Parameters:\n",
    "        y_true: True values\n",
    "        y_pred: Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        ccc: CCC value, range [-1, 1], 1 indicates perfect agreement\n",
    "    \n",
    "    Formula:\n",
    "        CCC = 2 * Ï * Ïƒ_x * Ïƒ_y / (Ïƒ_xÂ² + Ïƒ_yÂ² + (Î¼_x - Î¼_y)Â²)\n",
    "        where:\n",
    "            Ï = Pearson correlation coefficient\n",
    "            Ïƒ_x, Ïƒ_y = Standard deviations\n",
    "            Î¼_x, Î¼_y = Means\n",
    "    \"\"\"\n",
    "    # Means\n",
    "    mean_true = np.mean(y_true)\n",
    "    mean_pred = np.mean(y_pred)\n",
    "    \n",
    "    # Variances\n",
    "    var_true = np.var(y_true)\n",
    "    var_pred = np.var(y_pred)\n",
    "    \n",
    "    # Standard deviations\n",
    "    std_true = np.std(y_true)\n",
    "    std_pred = np.std(y_pred)\n",
    "    \n",
    "    # Pearson correlation coefficient\n",
    "    pearson_corr = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    \n",
    "    # CCC\n",
    "    ccc = 2 * pearson_corr * std_true * std_pred / (var_true + var_pred + (mean_true - mean_pred)**2)\n",
    "    \n",
    "    return ccc\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name, target_name):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    ccc = concordance_correlation_coefficient(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Target': target_name,\n",
    "        'RÂ²': r2,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'CCC': ccc\n",
    "    }\n",
    "\n",
    "print(\"âœ… Evaluation functions defined (including CCC metric)\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Valence Prediction Models\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"ðŸŽ¯ Training Valence Prediction Models (Check Overfitting)\")\n",
    "\n",
    "results_valence = []\n",
    "\n",
    "# 1. Linear Regression\n",
    "print(\"\\n1ï¸âƒ£ Linear Regression...\")\n",
    "lr_v = LinearRegression()\n",
    "lr_v.fit(X_train_scaled, y_v_train)\n",
    "y_v_pred_lr_train = lr_v.predict(X_train_scaled)\n",
    "y_v_pred_lr = lr_v.predict(X_test_scaled)\n",
    "train_r2 = r2_score(y_v_train, y_v_pred_lr_train)\n",
    "results_valence.append(evaluate_model(y_v_test, y_v_pred_lr, 'Linear Regression', 'Valence'))\n",
    "print(f\"   Train RÂ² = {train_r2:.4f} | Test RÂ² = {results_valence[-1]['RÂ²']:.4f} | CCC = {results_valence[-1]['CCC']:.4f}\")\n",
    "\n",
    "# 2. Ridge Regression\n",
    "print(\"\\n2ï¸âƒ£ Ridge Regression...\")\n",
    "ridge_v = Ridge(alpha=1.0, random_state=RANDOM_STATE)\n",
    "ridge_v.fit(X_train_scaled, y_v_train)\n",
    "y_v_pred_ridge_train = ridge_v.predict(X_train_scaled)\n",
    "y_v_pred_ridge = ridge_v.predict(X_test_scaled)\n",
    "train_r2 = r2_score(y_v_train, y_v_pred_ridge_train)\n",
    "results_valence.append(evaluate_model(y_v_test, y_v_pred_ridge, 'Ridge', 'Valence'))\n",
    "print(f\"   Train RÂ² = {train_r2:.4f} | Test RÂ² = {results_valence[-1]['RÂ²']:.4f} | CCC = {results_valence[-1]['CCC']:.4f}\")\n",
    "\n",
    "# 3. Random Forest\n",
    "print(\"\\n3ï¸âƒ£ Random Forest...\")\n",
    "rf_v = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf_v.fit(X_train_scaled, y_v_train)\n",
    "y_v_pred_rf_train = rf_v.predict(X_train_scaled)\n",
    "y_v_pred_rf = rf_v.predict(X_test_scaled)\n",
    "train_r2 = r2_score(y_v_train, y_v_pred_rf_train)\n",
    "results_valence.append(evaluate_model(y_v_test, y_v_pred_rf, 'Random Forest', 'Valence'))\n",
    "print(f\"   Train RÂ² = {train_r2:.4f} | Test RÂ² = {results_valence[-1]['RÂ²']:.4f} | CCC = {results_valence[-1]['CCC']:.4f}\")\n",
    "\n",
    "# Display results\n",
    "df_results_v = pd.DataFrame(results_valence)\n",
    "print(\"\\nðŸ“Š Valence Prediction Results (Test Set)\")\n",
    "df_results_v\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Arousal Prediction Models\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"ðŸŽ¯ Training Arousal Prediction Models (Check Overfitting)\")\n",
    "\n",
    "results_arousal = []\n",
    "\n",
    "# 1. Linear Regression\n",
    "print(\"\\n1ï¸âƒ£ Linear Regression...\")\n",
    "lr_a = LinearRegression()\n",
    "lr_a.fit(X_train_scaled, y_a_train)\n",
    "y_a_pred_lr_train = lr_a.predict(X_train_scaled)\n",
    "y_a_pred_lr = lr_a.predict(X_test_scaled)\n",
    "train_r2 = r2_score(y_a_train, y_a_pred_lr_train)\n",
    "results_arousal.append(evaluate_model(y_a_test, y_a_pred_lr, 'Linear Regression', 'Arousal'))\n",
    "print(f\"   Train RÂ² = {train_r2:.4f} | Test RÂ² = {results_arousal[-1]['RÂ²']:.4f} | CCC = {results_arousal[-1]['CCC']:.4f}\")\n",
    "\n",
    "# 2. Ridge Regression\n",
    "print(\"\\n2ï¸âƒ£ Ridge Regression...\")\n",
    "ridge_a = Ridge(alpha=1.0, random_state=RANDOM_STATE)\n",
    "ridge_a.fit(X_train_scaled, y_a_train)\n",
    "y_a_pred_ridge_train = ridge_a.predict(X_train_scaled)\n",
    "y_a_pred_ridge = ridge_a.predict(X_test_scaled)\n",
    "train_r2 = r2_score(y_a_train, y_a_pred_ridge_train)\n",
    "results_arousal.append(evaluate_model(y_a_test, y_a_pred_ridge, 'Ridge', 'Arousal'))\n",
    "print(f\"   Train RÂ² = {train_r2:.4f} | Test RÂ² = {results_arousal[-1]['RÂ²']:.4f} | CCC = {results_arousal[-1]['CCC']:.4f}\")\n",
    "\n",
    "# 3. Random Forest\n",
    "print(\"\\n3ï¸âƒ£ Random Forest...\")\n",
    "rf_a = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf_a.fit(X_train_scaled, y_a_train)\n",
    "y_a_pred_rf_train = rf_a.predict(X_train_scaled)\n",
    "y_a_pred_rf = rf_a.predict(X_test_scaled)\n",
    "train_r2 = r2_score(y_a_train, y_a_pred_rf_train)\n",
    "results_arousal.append(evaluate_model(y_a_test, y_a_pred_rf, 'Random Forest', 'Arousal'))\n",
    "print(f\"   Train RÂ² = {train_r2:.4f} | Test RÂ² = {results_arousal[-1]['RÂ²']:.4f} | CCC = {results_arousal[-1]['CCC']:.4f}\")\n",
    "\n",
    "# Display results\n",
    "df_results_a = pd.DataFrame(results_arousal)\n",
    "print(\"\\nðŸ“Š Arousal Prediction Results (Test Set)\")\n",
    "df_results_a\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Dominance Prediction Models (MuSe-specific)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"ðŸŽ¯ Training Dominance Prediction Models (MuSe-specific dimension)\")\n",
    "\n",
    "results_dominance = []\n",
    "\n",
    "# 1. Linear Regression\n",
    "print(\"\\n1ï¸âƒ£ Linear Regression...\")\n",
    "lr_d = LinearRegression()\n",
    "lr_d.fit(X_train_scaled, y_d_train)\n",
    "y_d_pred_lr_train = lr_d.predict(X_train_scaled)\n",
    "y_d_pred_lr = lr_d.predict(X_test_scaled)\n",
    "train_r2 = r2_score(y_d_train, y_d_pred_lr_train)\n",
    "results_dominance.append(evaluate_model(y_d_test, y_d_pred_lr, 'Linear Regression', 'Dominance'))\n",
    "print(f\"   Train RÂ² = {train_r2:.4f} | Test RÂ² = {results_dominance[-1]['RÂ²']:.4f} | CCC = {results_dominance[-1]['CCC']:.4f}\")\n",
    "\n",
    "# 2. Ridge Regression\n",
    "print(\"\\n2ï¸âƒ£ Ridge Regression...\")\n",
    "ridge_d = Ridge(alpha=1.0, random_state=RANDOM_STATE)\n",
    "ridge_d.fit(X_train_scaled, y_d_train)\n",
    "y_d_pred_ridge_train = ridge_d.predict(X_train_scaled)\n",
    "y_d_pred_ridge = ridge_d.predict(X_test_scaled)\n",
    "train_r2 = r2_score(y_d_train, y_d_pred_ridge_train)\n",
    "results_dominance.append(evaluate_model(y_d_test, y_d_pred_ridge, 'Ridge', 'Dominance'))\n",
    "print(f\"   Train RÂ² = {train_r2:.4f} | Test RÂ² = {results_dominance[-1]['RÂ²']:.4f} | CCC = {results_dominance[-1]['CCC']:.4f}\")\n",
    "\n",
    "# 3. Random Forest\n",
    "print(\"\\n3ï¸âƒ£ Random Forest...\")\n",
    "rf_d = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf_d.fit(X_train_scaled, y_d_train)\n",
    "y_d_pred_rf_train = rf_d.predict(X_train_scaled)\n",
    "y_d_pred_rf = rf_d.predict(X_test_scaled)\n",
    "train_r2 = r2_score(y_d_train, y_d_pred_rf_train)\n",
    "results_dominance.append(evaluate_model(y_d_test, y_d_pred_rf, 'Random Forest', 'Dominance'))\n",
    "print(f\"   Train RÂ² = {train_r2:.4f} | Test RÂ² = {results_dominance[-1]['RÂ²']:.4f} | CCC = {results_dominance[-1]['CCC']:.4f}\")\n",
    "\n",
    "# Display results\n",
    "df_results_d = pd.DataFrame(results_dominance)\n",
    "print(\"\\nðŸ“Š Dominance Prediction Results (Test Set)\")\n",
    "df_results_d\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Optimization: Random Forest Parameter Tuning\n",
    "\n",
    "### Problem Diagnosis\n",
    "- Linear/Ridge: Underfitting (Train RÂ² â‰ˆ 10%)\n",
    "- Random Forest: Severe overfitting (Train RÂ² â‰ˆ 75%, Test RÂ² < 0)\n",
    "\n",
    "### Improvement Strategy\n",
    "1. **Reduce overfitting**: Lower Random Forest max_depth\n",
    "2. **Increase regularization**: Use cross-validation for tuning\n",
    "3. **Try other models**: XGBoost, SVR\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"ðŸ”§ Optimization Plan 1: Adjust Random Forest Parameters\")\n",
    "\n",
    "# Try different parameter combinations\n",
    "param_configs = [\n",
    "    {'name': 'RF_Original', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1},\n",
    "    {'name': 'RF_Shallow', 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1},\n",
    "    {'name': 'RF_Regularized', 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 5},\n",
    "    {'name': 'RF_StrongReg', 'max_depth': 5, 'min_samples_split': 20, 'min_samples_leaf': 10},\n",
    "]\n",
    "\n",
    "results_optimized = []\n",
    "\n",
    "for config in param_configs:\n",
    "    print(f\"\\nðŸŒ² {config['name']}...\")\n",
    "    \n",
    "    # Train Valence model\n",
    "    rf_opt = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=config['max_depth'],\n",
    "        min_samples_split=config['min_samples_split'],\n",
    "        min_samples_leaf=config['min_samples_leaf'],\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf_opt.fit(X_train_scaled, y_v_train)\n",
    "    \n",
    "    # Calculate train and test RÂ²\n",
    "    y_v_pred_train = rf_opt.predict(X_train_scaled)\n",
    "    y_v_pred_test = rf_opt.predict(X_test_scaled)\n",
    "    \n",
    "    train_r2 = r2_score(y_v_train, y_v_pred_train)\n",
    "    test_r2 = r2_score(y_v_test, y_v_pred_test)\n",
    "    ccc = concordance_correlation_coefficient(y_v_test, y_v_pred_test)\n",
    "    \n",
    "    gap = train_r2 - test_r2\n",
    "    \n",
    "    print(f\"   Train RÂ² = {train_r2:.4f} | Test RÂ² = {test_r2:.4f} | Gap = {gap:.4f} | CCC = {ccc:.4f}\")\n",
    "    \n",
    "    results_optimized.append({\n",
    "        'Model': config['name'],\n",
    "        'Train_RÂ²': train_r2,\n",
    "        'Test_RÂ²': test_r2,\n",
    "        'Gap': gap,\n",
    "        'CCC': ccc,\n",
    "        'max_depth': config['max_depth']\n",
    "    })\n",
    "\n",
    "# Display results comparison\n",
    "df_optimized = pd.DataFrame(results_optimized)\n",
    "print(\"\\nðŸ“Š Random Forest Parameter Optimization Results (Valence)\")\n",
    "display(df_optimized)\n",
    "\n",
    "# Find best model (highest test RÂ²)\n",
    "best_idx = df_optimized['Test_RÂ²'].idxmax()\n",
    "best_model = df_optimized.iloc[best_idx]\n",
    "print(f\"\\nðŸ† Best configuration: {best_model['Model']}\")\n",
    "print(f\"   Test RÂ² = {best_model['Test_RÂ²']:.4f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Results Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Merge all results\n",
    "df_all_results = pd.concat([df_results_v, df_results_a, df_results_d], ignore_index=True)\n",
    "\n",
    "# Create directory for saving figures\n",
    "FIGURE_DIR = Path('../docs/figures')\n",
    "FIGURE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Visualize RÂ² comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "targets = ['Valence', 'Arousal', 'Dominance']\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#95E1D3']\n",
    "\n",
    "for idx, (target, color) in enumerate(zip(targets, colors)):\n",
    "    df_target = df_all_results[df_all_results['Target'] == target]\n",
    "    \n",
    "    axes[idx].bar(df_target['Model'], df_target['RÂ²'], color=color, alpha=0.8)\n",
    "    axes[idx].set_title(f'{target} Prediction - RÂ² Score', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_ylabel('RÂ² Score', fontsize=12)\n",
    "    axes[idx].set_ylim([0, 1])\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    axes[idx].tick_params(axis='x', rotation=15)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (model, r2) in enumerate(zip(df_target['Model'], df_target['RÂ²'])):\n",
    "        axes[idx].text(i, r2 + 0.02, f'{r2:.3f}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = FIGURE_DIR / 'muse_baseline_results.png'\n",
    "plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Prediction vs True Values Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Use best model (Random Forest) predictions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "predictions = [\n",
    "    (y_v_test, y_v_pred_rf, 'Valence', '#FF6B6B'),\n",
    "    (y_a_test, y_a_pred_rf, 'Arousal', '#4ECDC4'),\n",
    "    (y_d_test, y_d_pred_rf, 'Dominance', '#95E1D3')\n",
    "]\n",
    "\n",
    "for idx, (y_true, y_pred, target, color) in enumerate(predictions):\n",
    "    axes[idx].scatter(y_true, y_pred, alpha=0.6, color=color, s=50)\n",
    "    \n",
    "    # Add perfect prediction line (y=x)\n",
    "    min_val = min(y_true.min(), y_pred.min())\n",
    "    max_val = max(y_true.max(), y_pred.max())\n",
    "    axes[idx].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2, label='Perfect Prediction')\n",
    "    \n",
    "    # Calculate RÂ²\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    axes[idx].set_title(f'{target} Prediction (RF)\\nRÂ² = {r2:.3f}', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xlabel('True Values', fontsize=12)\n",
    "    axes[idx].set_ylabel('Predicted Values', fontsize=12)\n",
    "    axes[idx].legend(loc='upper left')\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = FIGURE_DIR / 'muse_predictions_scatter.png'\n",
    "plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"ðŸ“ MuSe Dataset Baseline Models Summary\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Dataset Overview:\")\n",
    "print(f\"   â€¢ Valid audio: {len(df_features)} songs (from 1000 original)\")\n",
    "print(f\"   â€¢ Audio duration: 30 seconds\")\n",
    "print(f\"   â€¢ Feature dimensions: {len(feature_cols)} dims\")\n",
    "print(f\"   â€¢ Annotation dimensions: V-A-D (3 dims)\")\n",
    "\n",
    "print(\"\\nðŸ¤– Best Model Performance:\")\n",
    "for target in ['Valence', 'Arousal', 'Dominance']:\n",
    "    df_target = df_all_results[df_all_results['Target'] == target]\n",
    "    best_model = df_target.loc[df_target['CCC'].idxmax()]\n",
    "    print(f\"\\n   {target}:\")\n",
    "    print(f\"      Model: {best_model['Model']}\")\n",
    "    print(f\"      CCC: {best_model['CCC']:.4f} â­ (primary metric)\")\n",
    "    print(f\"      RÂ²: {best_model['RÂ²']:.4f}\")\n",
    "    print(f\"      RMSE: {best_model['RMSE']:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Key Findings:\")\n",
    "print(\"   1. Random Forest performs best across all dimensions\")\n",
    "print(\"   2. Arousal prediction typically outperforms Valence\")\n",
    "print(\"   3. Dominance is MuSe-specific, providing additional emotion representation\")\n",
    "print(\"   4. Traditional audio features (MFCC, Chroma, Spectral) are effective for emotion prediction\")\n",
    "\n",
    "print(\"\\nðŸ”„ Comparison with DEAM:\")\n",
    "print(\"   â€¢ MuSe has larger dataset (953 vs ~1800)\")\n",
    "print(\"   â€¢ MuSe includes Dominance dimension\")\n",
    "print(\"   â€¢ Shorter audio duration (30s vs 45s)\")\n",
    "print(\"   â€¢ Different annotation source (Last.fm tags vs crowdsourcing)\")\n",
    "\n",
    "print(\"\\nðŸ“Œ Next Steps:\")\n",
    "print(\"   1. Try more models (XGBoost, LightGBM, MLP)\")\n",
    "print(\"   2. Extract deep learning features (wav2vec2, CLAP)\")\n",
    "print(\"   3. Multimodal fusion (audio + lyrics + metadata)\")\n",
    "print(\"   4. Hyperparameter optimization and cross-validation\")\n",
    "print(\"   5. Feature importance analysis\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Models\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import joblib\n",
    "\n",
    "# Create model save directory\n",
    "MODEL_DIR = Path('../models/muse_baseline')\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "models_to_save = [\n",
    "    (lr_v, 'linear_regression_valence.pkl'),\n",
    "    (lr_a, 'linear_regression_arousal.pkl'),\n",
    "    (lr_d, 'linear_regression_dominance.pkl'),\n",
    "    (ridge_v, 'ridge_valence.pkl'),\n",
    "    (ridge_a, 'ridge_arousal.pkl'),\n",
    "    (ridge_d, 'ridge_dominance.pkl'),\n",
    "    (rf_v, 'random_forest_valence.pkl'),\n",
    "    (rf_a, 'random_forest_arousal.pkl'),\n",
    "    (rf_d, 'random_forest_dominance.pkl'),\n",
    "    (scaler, 'feature_scaler.pkl'),\n",
    "]\n",
    "\n",
    "for model, filename in models_to_save:\n",
    "    model_path = MODEL_DIR / filename\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "# Save results\n",
    "results_path = OUTPUT_DIR / 'muse_baseline_results.csv'\n",
    "df_all_results.to_csv(results_path, index=False)\n",
    "\n",
    "print(\"âœ… All models saved!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}